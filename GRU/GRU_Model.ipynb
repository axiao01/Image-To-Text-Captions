{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ef9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================== 1) Imports & Config ==========================\n",
    "import os, zipfile, tempfile, requests, time, random\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class Config:\n",
    "    BATCH_SIZE = 32             # CPU friendly\n",
    "    NUM_EPOCHS = 20\n",
    "    EMBED_SIZE = 512\n",
    "    HIDDEN_SIZE = 512\n",
    "    DROPOUT = 0.3\n",
    "    LR = 1e-4\n",
    "    VOCAB_THRESHOLD = 5\n",
    "\n",
    "    DEVICE = torch.device('cpu')  # force CPU\n",
    "\n",
    "    # Will be created under a temp dir by prepare_data()\n",
    "    IMAGES_DIR = None\n",
    "    CAPTIONS_FILE = None\n",
    "    CACHED_FEATURES_DIR = None\n",
    "\n",
    "print(\"Using device:\", Config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abd7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 2) Vocab ==========================\n",
    "class Vocabulary:\n",
    "    def __init__(self, threshold=5):\n",
    "        self.threshold = threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        for tok in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "            self.add_word(tok)\n",
    "\n",
    "    def add_word(self, w):\n",
    "        if w not in self.word2idx:\n",
    "            self.word2idx[w] = self.idx\n",
    "            self.idx2word[self.idx] = w\n",
    "            self.idx += 1\n",
    "\n",
    "    def build(self, captions_list):\n",
    "        counter = Counter()\n",
    "        for cap in tqdm(captions_list, desc=\"Building Vocabulary\"):\n",
    "            counter.update(word_tokenize(cap.lower()))\n",
    "        for w, c in counter.items():\n",
    "            if c >= self.threshold:\n",
    "                self.add_word(w)\n",
    "        print(f\"Vocabulary size: {len(self)}\")\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.word2idx.get(w, self.word2idx['<unk>'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ee2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 3) Encoder (ResNet-50 -> 2048) ==========================\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(resnet, return_nodes={'layer4':'feat'})\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.backbone(images)['feat']          # (B, 2048, H, W)\n",
    "        x = self.pool(x)                           # (B, 2048, 1, 1)\n",
    "        return x.view(x.size(0), -1)               # (B, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c23aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 4) Captions parser ==========================\n",
    "def parse_captions(captions_file):\n",
    "    ann = {}\n",
    "    with open(captions_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) < 2:\n",
    "                parts = line.split(' ', 1)\n",
    "                if len(parts) < 2: continue\n",
    "            image_caption_id, caption = parts[0].strip(), parts[1].strip()\n",
    "            img_id = image_caption_id.split('#')[0]\n",
    "            if not img_id.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            ann.setdefault(img_id, []).append(caption)\n",
    "    image_ids = list(ann.keys())\n",
    "    return ann, image_ids\n",
    "\n",
    "# ========================== 5) Simple feature caching (no DataLoader) ==========================\n",
    "def cache_features_naive(image_ids, images_dir, out_dir, transform):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    enc = EncoderCNN().to('cpu').eval()\n",
    "    for img_id in tqdm(image_ids, desc=\"Caching features (simple loop)\"):\n",
    "        out_path = os.path.join(out_dir, img_id.replace('.jpg', '.pt'))\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        img_path = os.path.join(images_dir, img_id)\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                x = transform(im.convert('RGB')).unsqueeze(0)  # (1,3,224,224)\n",
    "            with torch.inference_mode():\n",
    "                feat = enc(x).squeeze(0).cpu()                 # (2048,)\n",
    "            torch.save(feat, out_path)\n",
    "        except Exception as e:\n",
    "            # skip corrupt/missing image\n",
    "            # print(f\"Skip {img_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "# ========================== 6) Dataset that reads cached features ==========================\n",
    "class Flickr8kCached(Dataset):\n",
    "    def __init__(self, image_ids, annotations, vocab, cached_dir):\n",
    "        self.image_ids = image_ids\n",
    "        self.annotations = annotations\n",
    "        self.vocab = vocab\n",
    "        self.cached_dir = cached_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids) * 5  # 5 caps per image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_i = idx // 5\n",
    "        cap_i = idx % 5\n",
    "        img_id = self.image_ids[img_i]\n",
    "        feat_path = os.path.join(self.cached_dir, img_id.replace('.jpg', '.pt'))\n",
    "        feat = torch.load(feat_path)        # (2048,)\n",
    "        # tokenise caption\n",
    "        tokens = word_tokenize(self.annotations[img_id][cap_i].lower())\n",
    "        seq = [self.vocab('<start>')] + [self.vocab(t) for t in tokens] + [self.vocab('<end>')]\n",
    "        return feat, torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    feats, caps = zip(*batch)              # feats: list of (2048,)\n",
    "    feats = torch.stack(feats, 0)\n",
    "    lengths = [len(c) for c in caps]\n",
    "    targets = torch.zeros(len(caps), max(lengths), dtype=torch.long)\n",
    "    for i, c in enumerate(caps):\n",
    "        targets[i, :len(c)] = c\n",
    "    return feats, targets, torch.tensor(lengths, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebea4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 7) GRU Decoder (train + step for inference) ==========================\n",
    "class DecoderWithGRU(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, encoder_dim=2048, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.embed.weight.data.uniform_(-0.1,0.1)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-0.1,0.1)\n",
    "\n",
    "    def init_hidden_state(self, enc_out):   # enc_out: (B,2048) or (B,1,2048)\n",
    "        if enc_out.dim()==3: enc_out = enc_out.squeeze(1)\n",
    "        h0 = self.init_h(enc_out).unsqueeze(0)  # (1,B,H)\n",
    "        return h0\n",
    "\n",
    "    # one-step for decoding\n",
    "    def step(self, word_idx, h):\n",
    "        # word_idx: (B,) Long\n",
    "        emb = self.embed(word_idx).unsqueeze(1)    # (B,1,E)\n",
    "        out, h_new = self.gru(emb, h)              # out: (B,1,H)\n",
    "        logits = self.fc(out.squeeze(1))           # (B,V)\n",
    "        return logits, h_new\n",
    "\n",
    "    # training forward with packed sequences\n",
    "    def forward(self, enc_feats, caps, caplens):\n",
    "        if enc_feats.dim()==2:\n",
    "            enc_feats = enc_feats.unsqueeze(1)\n",
    "        # sort by length\n",
    "        caplens = caplens.squeeze(-1) if caplens.dim()>1 else caplens\n",
    "        caplens, sort_idx = caplens.sort(0, descending=True)\n",
    "        enc_feats = enc_feats[sort_idx]\n",
    "        caps = caps[sort_idx]\n",
    "\n",
    "        h0 = self.init_hidden_state(enc_feats)     # (1,B,H)\n",
    "        emb = self.drop(self.embed(caps))          # (B,T,E)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, caplens.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_out, _ = self.gru(packed, h0)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,H)\n",
    "        logits = self.fc(out)                      # (B,T,V)\n",
    "        decode_lengths = (caplens - 1).tolist()\n",
    "        return logits, caps, decode_lengths, sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583c277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 8) Beam search & BLEU on cached features ==========================\n",
    "def beam_search(decoder, img_feat, vocab, beam_size=5, max_len=20):\n",
    "    k = beam_size\n",
    "    start = vocab.word2idx['<start>']; end = vocab.word2idx['<end>']\n",
    "    enc = img_feat.unsqueeze(1)                    # (1,1,2048)\n",
    "    h = decoder.init_hidden_state(enc)             # (1,1,H)\n",
    "\n",
    "    beams = [([], h, 0.0)]                         # (seq, h, logp)\n",
    "    for _ in range(max_len):\n",
    "        cand = []\n",
    "        for seq, h_cur, score in beams:\n",
    "            if seq and seq[-1]==end:\n",
    "                cand.append((seq, h_cur, score)); continue\n",
    "            last = torch.tensor([seq[-1] if seq else start], dtype=torch.long)\n",
    "            logits, h_next = decoder.step(last, h_cur)\n",
    "            logp = torch.log_softmax(logits, dim=-1)\n",
    "            topk_lp, topk_idx = torch.topk(logp, k, dim=-1)\n",
    "            for i in range(k):\n",
    "                cand.append((seq+[topk_idx[0,i].item()], h_next, score+topk_lp[0,i].item()))\n",
    "        beams = sorted(cand, key=lambda x:x[2], reverse=True)[:k]\n",
    "        if all(s and s[-1]==end for s,_,_ in beams): break\n",
    "    return beams[0][0]\n",
    "\n",
    "def evaluate_bleu_features(decoder, val_dataset, vocab, beam_size=5, max_len=20):\n",
    "    sf = SmoothingFunction()\n",
    "    decoder.eval()\n",
    "    scores=[]\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(val_dataset.image_ids, desc=\"BLEU eval (cached)\"):\n",
    "            feat = torch.load(os.path.join(val_dataset.cached_dir, img_id.replace('.jpg','.pt'))).unsqueeze(0)\n",
    "            seq = beam_search(decoder, feat, vocab, beam_size=beam_size, max_len=max_len)\n",
    "            pred = [vocab.idx2word[i] for i in seq if i not in {vocab.word2idx['<start>'],vocab.word2idx['<end>'],vocab.word2idx['<pad>']}]\n",
    "            refs = [word_tokenize(c.lower()) for c in val_dataset.annotations[img_id]]\n",
    "            scores.append(sentence_bleu(refs, pred, smoothing_function=sf.method4))\n",
    "    return float(np.mean(scores)) if scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad09b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 9) Download Flickr8k ==========================\n",
    "def download_flickr8k():\n",
    "    tmp = tempfile.mkdtemp(prefix=\"flickr8k_\")\n",
    "    cap_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "    img_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "    cap_zip = os.path.join(tmp, \"caps.zip\")\n",
    "    img_zip = os.path.join(tmp, \"imgs.zip\")\n",
    "\n",
    "    for url, out in [(cap_url, cap_zip), (img_url, img_zip)]:\n",
    "        if not os.path.exists(out):\n",
    "            r = requests.get(url, stream=True, timeout=300)\n",
    "            r.raise_for_status()\n",
    "            with open(out, \"wb\") as f:\n",
    "                for chunk in r.iter_content(8192): f.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(cap_zip, 'r') as z: z.extractall(tmp)\n",
    "    with zipfile.ZipFile(img_zip, 'r') as z: z.extractall(tmp)\n",
    "\n",
    "    Config.CAPTIONS_FILE = os.path.join(tmp, \"Flickr8k.token.txt\")\n",
    "    Config.IMAGES_DIR = os.path.join(tmp, \"Flicker8k_Dataset\")\n",
    "    Config.CACHED_FEATURES_DIR = os.path.join(tmp, \"cached_features\")\n",
    "    os.makedirs(Config.CACHED_FEATURES_DIR, exist_ok=True)\n",
    "    print(\"Data at:\", tmp)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407c3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 10) Prepare data ==========================\n",
    "def prepare_data():\n",
    "    root = download_flickr8k()\n",
    "    annotations, image_ids = parse_captions(Config.CAPTIONS_FILE)\n",
    "\n",
    "    # Build vocab\n",
    "    all_caps = [c for caps in annotations.values() for c in caps]\n",
    "    vocab = Vocabulary(Config.VOCAB_THRESHOLD)\n",
    "    vocab.build(all_caps)\n",
    "\n",
    "    # Split first (so we only cache what we need if you want)\n",
    "    train_ids, val_ids = train_test_split(image_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Transform for encoder\n",
    "    tx = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    # Cache features (simple loop; CPU)\n",
    "    # You can cache train+val together:\n",
    "    ids_to_cache = train_ids + val_ids\n",
    "    cache_features_naive(ids_to_cache, Config.IMAGES_DIR, Config.CACHED_FEATURES_DIR, tx)\n",
    "\n",
    "    # Build datasets that read cached features only\n",
    "    train_ann = {i: annotations[i] for i in train_ids}\n",
    "    val_ann   = {i: annotations[i] for i in val_ids}\n",
    "    train_ds = Flickr8kCached(train_ids, train_ann, vocab, Config.CACHED_FEATURES_DIR)\n",
    "    val_ds   = Flickr8kCached(val_ids,   val_ann,   vocab, Config.CACHED_FEATURES_DIR)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    # Model + optim\n",
    "    decoder = DecoderWithGRU(Config.EMBED_SIZE, Config.HIDDEN_SIZE, len(vocab), dropout=Config.DROPOUT).to(Config.DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<pad>'])\n",
    "    optimizer = optim.Adam(decoder.parameters(), lr=Config.LR)\n",
    "\n",
    "    return decoder, optimizer, criterion, vocab, train_ds, val_ds, train_loader, val_loader, root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c19d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== 11) Train / Validate / BLEU ==========================\n",
    "def train_one_epoch(decoder, optimizer, criterion, loader, device='cpu'):\n",
    "    decoder.train()\n",
    "    running = 0.0\n",
    "    for feats, caps, lens in tqdm(loader, desc=\"Training\"):\n",
    "        feats, caps, lens = feats.to(device), caps.to(device), lens.to(device)\n",
    "        logits, caps_sorted, decode_lengths, _ = decoder(feats, caps, lens)\n",
    "        targets = caps_sorted[:,1:]\n",
    "        loss = 0.0\n",
    "        for i, L in enumerate(decode_lengths):\n",
    "            loss += criterion(logits[i,:L,:], targets[i,:L])\n",
    "        loss /= len(decode_lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        running += loss.item()\n",
    "    return running/len(loader)\n",
    "\n",
    "def validate(decoder, criterion, loader, device='cpu'):\n",
    "    decoder.eval()\n",
    "    running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feats, caps, lens in tqdm(loader, desc=\"Validation\"):\n",
    "            feats, caps, lens = feats.to(device), caps.to(device), lens.to(device)\n",
    "            logits, caps_sorted, decode_lengths, _ = decoder(feats, caps, lens)\n",
    "            targets = caps_sorted[:,1:]\n",
    "            loss = 0.0\n",
    "            for i, L in enumerate(decode_lengths):\n",
    "                loss += criterion(logits[i,:L,:], targets[i,:L])\n",
    "            loss /= len(decode_lengths)\n",
    "            running += loss.item()\n",
    "    return running/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee01e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at: C:\\Users\\jocel\\AppData\\Local\\Temp\\flickr8k_ejxzg7dy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Vocabulary: 100%|██████████| 40455/40455 [00:23<00:00, 1705.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching features (simple loop): 100%|██████████| 8091/8091 [54:16<00:00,  2.48it/s]  \n"
     ]
    }
   ],
   "source": [
    "decoder, optimizer, criterion, vocab, train_ds, val_ds, train_loader, val_loader, _tmp = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c309a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 7281, Val images: 810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [11:10<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:42<00:00,  2.97it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:29<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train 4.086 | Val 3.420 | BLEU 0.1775 | 13.4 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:04<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  7.98it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:38<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train 3.181 | Val 3.055 | BLEU 0.1857 | 11.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:14<00:00,  2.05it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:12<00:00, 10.01it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:30<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train 2.882 | Val 2.886 | BLEU 0.2119 | 11.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:35<00:00,  2.21it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:18<00:00,  6.89it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:23<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train 2.704 | Val 2.787 | BLEU 0.2100 | 10.3 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:48<00:00,  2.15it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:17<00:00,  7.28it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:17<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train 2.575 | Val 2.723 | BLEU 0.2191 | 10.4 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:52<00:00,  2.14it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  8.02it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:43<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train 2.471 | Val 2.675 | BLEU 0.2181 | 10.9 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:58<00:00,  2.11it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:16<00:00,  7.89it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:25<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train 2.384 | Val 2.640 | BLEU 0.2143 | 10.7 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:22<00:00,  2.02it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:17<00:00,  7.19it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:16<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train 2.310 | Val 2.615 | BLEU 0.2201 | 10.9 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:56<00:00,  2.12it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:18<00:00,  6.77it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:28<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train 2.243 | Val 2.595 | BLEU 0.2253 | 10.7 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:41<00:00,  2.18it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:18<00:00,  6.85it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:27<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train 2.183 | Val 2.584 | BLEU 0.2243 | 10.5 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:47<00:00,  2.16it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:18<00:00,  6.80it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:29<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train 2.128 | Val 2.571 | BLEU 0.2218 | 10.6 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:16<00:00,  2.05it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:16<00:00,  7.49it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:35<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train 2.076 | Val 2.568 | BLEU 0.2152 | 11.1 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [08:45<00:00,  2.17it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:13<00:00,  9.59it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:30<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train 2.028 | Val 2.559 | BLEU 0.2194 | 10.5 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:05<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:16<00:00,  7.50it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:43<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train 1.983 | Val 2.561 | BLEU 0.2266 | 11.1 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:03<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  8.29it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:42<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train 1.940 | Val 2.561 | BLEU 0.2184 | 11.0 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:50<00:00,  1.93it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  8.34it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:46<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train 1.899 | Val 2.564 | BLEU 0.2146 | 11.9 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:05<00:00,  2.09it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  8.02it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train 1.860 | Val 2.568 | BLEU 0.2089 | 10.8 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:46<00:00,  1.94it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:19<00:00,  6.56it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:21<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train 1.821 | Val 2.565 | BLEU 0.2152 | 11.4 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:24<00:00,  2.02it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:20<00:00,  6.18it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:35<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train 1.785 | Val 2.574 | BLEU 0.2133 | 11.3 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1138/1138 [09:48<00:00,  1.93it/s]\n",
      "Validation: 100%|██████████| 127/127 [00:15<00:00,  8.12it/s]\n",
      "BLEU eval (cached): 100%|██████████| 810/810 [01:29<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train 1.750 | Val 2.578 | BLEU 0.2042 | 11.6 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================== 12) Run ==========================\n",
    "print(f\"Train images: {len(train_ds.image_ids)}, Val images: {len(val_ds.image_ids)}\")\n",
    "\n",
    "best_bleu = 0.0\n",
    "for epoch in range(1, Config.NUM_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(decoder, optimizer, criterion, train_loader)\n",
    "    va_loss = validate(decoder, criterion, val_loader)\n",
    "    # Full-val BLEU with beam search (k=5)\n",
    "    bleu = evaluate_bleu_features(decoder, val_ds, vocab, beam_size=5, max_len=20)\n",
    "\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        torch.save({'decoder': decoder.state_dict(),\n",
    "                    'vocab_size': len(vocab)}, f'best_decoder_epoch{epoch}.pth')\n",
    "    dt = time.time()-t0\n",
    "    print(f\"Epoch {epoch}/{Config.NUM_EPOCHS} | Train {tr_loss:.3f} | Val {va_loss:.3f} | BLEU {bleu:.4f} | {dt/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96044b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
